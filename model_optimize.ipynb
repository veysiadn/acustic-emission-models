{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and USE GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from numpy import set_printoptions\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pathlib\n",
    "\n",
    "# Using GPU\n",
    "import os\n",
    "import scipy.io as scpy\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'  # Set to -1 if CPU should be used CPU = -1 , GPU = 0\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "cpus = tf.config.experimental.list_physical_devices('CPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "elif cpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        logical_cpus= tf.config.experimental.list_logical_devices('CPU')\n",
    "        print(len(cpus), \"Physical CPU,\", len(logical_cpus), \"Logical CPU\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 1000)\n",
      "(15000, 1)\n",
      " (12000, 1500, 1500)\n",
      " (12000, 1500, 1500)\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from numpy import set_printoptions\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "# Using GPU\n",
    "import os\n",
    "import scipy.io as scpy\n",
    "\n",
    "data = scpy.loadmat(\"all-data.mat\")\n",
    "# Extracting x_train from the mat file dictionary.\n",
    "x_data = data[\"XTrain\"]\n",
    "# Extracting y_train from the mat file dictionary.\n",
    "y_data = data[\"y_train\"]\n",
    "# Converting x_train and y_train to a numpy array.\n",
    "x_data = np.array(x_data,dtype='float32')\n",
    "y_data = np.array(y_data,dtype='float32')-1\n",
    "x_temp_data=data['XTest']\n",
    "y_temp_data=data['y_test']\n",
    "x_temp_data=np.array(x_temp_data,dtype='float32')\n",
    "y_temp_data=np.array(y_temp_data,dtype='float32')-1\n",
    "# x_data=np.concatenate((x_data,x_temp_data),axis=0)\n",
    "# y_data=np.concatenate((y_data,y_temp_data),axis=0)\n",
    "\n",
    "# Verifying the shapes.\n",
    "print(x_data.shape)\n",
    "print(y_data.shape)\n",
    "\n",
    "SEED = 99\n",
    "os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "# split into train test and validation sets\n",
    "x_train, x_val_to_use, y_train, y_val_to_use = train_test_split(x_data, y_data, test_size=0.2, random_state=SEED)\n",
    "x_val = x_val_to_use[:int(len(x_val_to_use)/2)]\n",
    "y_val = y_val_to_use[:int(len(y_val_to_use)/2)]\n",
    "x_test = x_val_to_use[int(len(x_val_to_use)/2):]\n",
    "y_test = y_val_to_use[int(len(y_val_to_use)/2):]\n",
    "print(f\" {len(x_train), len(x_val), len(x_test)}\")\n",
    "print(f\" {len(y_train), len(y_val), len(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arange dataset in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "BATCH_SIZE = 64\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check dimension of train, test and validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 1500, 1500)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train), len(x_val), len(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dense model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_4 (Flatten)         (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 4)                 4004      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 3)                 15        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,019\n",
      "Trainable params: 4,019\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      " 32/375 [=>............................] - ETA: 0s - loss: 1.0583 - accuracy: 0.4805 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/veysiadn/anaconda3/lib/python3.9/site-packages/keras/backend.py:5582: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 1s 3ms/step - loss: 0.6795 - accuracy: 0.7294 - val_loss: 0.4640 - val_accuracy: 0.8427\n",
      "Epoch 2/10\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3565 - accuracy: 0.8929 - val_loss: 0.2631 - val_accuracy: 0.9340\n",
      "Epoch 3/10\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2128 - accuracy: 0.9423 - val_loss: 0.1735 - val_accuracy: 0.9540\n",
      "Epoch 4/10\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1475 - accuracy: 0.9606 - val_loss: 0.1308 - val_accuracy: 0.9613\n",
      "Epoch 5/10\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1131 - accuracy: 0.9712 - val_loss: 0.1082 - val_accuracy: 0.9707\n",
      "Epoch 6/10\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0931 - accuracy: 0.9770 - val_loss: 0.0948 - val_accuracy: 0.9760\n",
      "Epoch 7/10\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0801 - accuracy: 0.9805 - val_loss: 0.0864 - val_accuracy: 0.9773\n",
      "Epoch 8/10\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0712 - accuracy: 0.9826 - val_loss: 0.0810 - val_accuracy: 0.9793\n",
      "Epoch 9/10\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0646 - accuracy: 0.9846 - val_loss: 0.0770 - val_accuracy: 0.9787\n",
      "Epoch 10/10\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0596 - accuracy: 0.9860 - val_loss: 0.0742 - val_accuracy: 0.9800\n",
      "Training results\n",
      "Training Accuracy: 0.9860000014305115\n",
      "Training Loss: 0.05955108627676964\n",
      "Evaluation results\n",
      "Validation Accuracy: 0.9800000190734863\n",
      "Validation Loss: 0.07416583597660065\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "SEED = 99\n",
    "os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "dense_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(1000,)),\n",
    "    tf.keras.layers.Dense(4, activation='elu'),\n",
    "    tf.keras.layers.Dense(3,activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "dense_model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "dense_model.summary()              \n",
    "\n",
    "history=dense_model.fit(train_dataset,validation_data=val_dataset, epochs=10)\n",
    "# dense_model.evaluate(val_dataset)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "\n",
    "\n",
    "plt.title('dense_model loss & accuracy')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['tr_loss', 'tr_accuracy', 'val_acc', 'val_loss'], loc='lower right')\n",
    "# accuracy!\n",
    "print(\"Training results\")\n",
    "print(f\"Training Accuracy: {history.history['accuracy'][-1]}\")\n",
    "print(f\"Training Loss: {history.history['loss'][-1]}\")\n",
    "\n",
    "# evaluating dense_model\n",
    "print(\"Evaluation results\")\n",
    "print(f\"Validation Accuracy: {history.history['val_accuracy'][-1]}\")\n",
    "print(f\"Validation Loss: {history.history['val_loss'][-1]}\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Only some Keras activations under `tf.keras.activations` are supported. For other activations, use `Quantizer` directly, and update layer config using `QuantizeConfig`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/veysiadn/model-training/acustic-emission-models/model_optimize.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/veysiadn/model-training/acustic-emission-models/model_optimize.ipynb#X40sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m quantize_model \u001b[39m=\u001b[39m tfmot\u001b[39m.\u001b[39mquantization\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mquantize_model\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/veysiadn/model-training/acustic-emission-models/model_optimize.ipynb#X40sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# q_aware stands for for quantization aware.\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/veysiadn/model-training/acustic-emission-models/model_optimize.ipynb#X40sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m q_aware_model \u001b[39m=\u001b[39m quantize_model(dense_model)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/veysiadn/model-training/acustic-emission-models/model_optimize.ipynb#X40sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# `quantize_model` requires a recompile.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/veysiadn/model-training/acustic-emission-models/model_optimize.ipynb#X40sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m q_aware_model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrmsprop\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/veysiadn/model-training/acustic-emission-models/model_optimize.ipynb#X40sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m               loss\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mSparseCategoricalCrossentropy(from_logits\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/veysiadn/model-training/acustic-emission-models/model_optimize.ipynb#X40sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m               metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow_model_optimization/python/core/quantization/keras/quantize.py:141\u001b[0m, in \u001b[0;36mquantize_model\u001b[0;34m(to_quantize)\u001b[0m\n\u001b[1;32m    136\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    137\u001b[0m       \u001b[39m'\u001b[39m\u001b[39m`to_quantize` can only either be a tf.keras Sequential or \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    138\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mFunctional model.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    140\u001b[0m annotated_model \u001b[39m=\u001b[39m quantize_annotate_model(to_quantize)\n\u001b[0;32m--> 141\u001b[0m \u001b[39mreturn\u001b[39;00m quantize_apply(annotated_model)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow_model_optimization/python/core/keras/metrics.py:74\u001b[0m, in \u001b[0;36mMonitorBoolGauge.__call__.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m error:\n\u001b[1;32m     73\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbool_gauge\u001b[39m.\u001b[39mget_cell(MonitorBoolGauge\u001b[39m.\u001b[39m_FAILURE_LABEL)\u001b[39m.\u001b[39mset(\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 74\u001b[0m   \u001b[39mraise\u001b[39;00m error\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow_model_optimization/python/core/keras/metrics.py:69\u001b[0m, in \u001b[0;36mMonitorBoolGauge.__call__.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     67\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     68\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m     results \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     70\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbool_gauge\u001b[39m.\u001b[39mget_cell(MonitorBoolGauge\u001b[39m.\u001b[39m_SUCCESS_LABEL)\u001b[39m.\u001b[39mset(\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     71\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow_model_optimization/python/core/quantization/keras/quantize.py:479\u001b[0m, in \u001b[0;36mquantize_apply\u001b[0;34m(model, scheme)\u001b[0m\n\u001b[1;32m    473\u001b[0m quantize_registry \u001b[39m=\u001b[39m scheme\u001b[39m.\u001b[39mget_quantize_registry()\n\u001b[1;32m    475\u001b[0m \u001b[39m# 4. Actually quantize all the relevant layers in the model. This is done by\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[39m# wrapping the layers with QuantizeWrapper, and passing the associated\u001b[39;00m\n\u001b[1;32m    477\u001b[0m \u001b[39m# `QuantizeConfig`.\u001b[39;00m\n\u001b[0;32m--> 479\u001b[0m \u001b[39mreturn\u001b[39;00m keras\u001b[39m.\u001b[39;49mmodels\u001b[39m.\u001b[39;49mclone_model(\n\u001b[1;32m    480\u001b[0m     transformed_model, input_tensors\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, clone_function\u001b[39m=\u001b[39;49m_quantize)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/models/cloning.py:501\u001b[0m, in \u001b[0;36mclone_model\u001b[0;34m(model, input_tensors, clone_function)\u001b[0m\n\u001b[1;32m    498\u001b[0m     clone_function \u001b[39m=\u001b[39m _clone_layer\n\u001b[1;32m    500\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(model, Sequential):\n\u001b[0;32m--> 501\u001b[0m     \u001b[39mreturn\u001b[39;00m _clone_sequential_model(\n\u001b[1;32m    502\u001b[0m         model, input_tensors\u001b[39m=\u001b[39;49minput_tensors, layer_fn\u001b[39m=\u001b[39;49mclone_function\n\u001b[1;32m    503\u001b[0m     )\n\u001b[1;32m    504\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    505\u001b[0m     \u001b[39mreturn\u001b[39;00m _clone_functional_model(\n\u001b[1;32m    506\u001b[0m         model, input_tensors\u001b[39m=\u001b[39minput_tensors, layer_fn\u001b[39m=\u001b[39mclone_function\n\u001b[1;32m    507\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/models/cloning.py:371\u001b[0m, in \u001b[0;36m_clone_sequential_model\u001b[0;34m(model, input_tensors, layer_fn)\u001b[0m\n\u001b[1;32m    366\u001b[0m layers, ancillary_layers \u001b[39m=\u001b[39m _remove_ancillary_layers(\n\u001b[1;32m    367\u001b[0m     model, layer_map, layers\n\u001b[1;32m    368\u001b[0m )\n\u001b[1;32m    370\u001b[0m \u001b[39mif\u001b[39;00m input_tensors \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 371\u001b[0m     cloned_model \u001b[39m=\u001b[39m Sequential(layers\u001b[39m=\u001b[39;49mlayers, name\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m    372\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(generic_utils\u001b[39m.\u001b[39mto_list(input_tensors)) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    373\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    374\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTo clone a `Sequential` model, we expect at most one tensor as \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    375\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpart of `input_tensors`. Received: input_tensors=\u001b[39m\u001b[39m{\u001b[39;00minput_tensors\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    376\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/trackable/base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    206\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow_model_optimization/python/core/quantization/keras/quantize_wrapper.py:240\u001b[0m, in \u001b[0;36mQuantizeWrapperV2.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild\u001b[39m(\u001b[39mself\u001b[39m, input_shape):\n\u001b[1;32m    239\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trainable_weights\u001b[39m.\u001b[39mextend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer\u001b[39m.\u001b[39mtrainable_weights)\n\u001b[0;32m--> 240\u001b[0m   \u001b[39msuper\u001b[39;49m(QuantizeWrapperV2, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mbuild(input_shape)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow_model_optimization/python/core/quantization/keras/quantize_wrapper.py:115\u001b[0m, in \u001b[0;36mQuantizeWrapper.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_quantize_activations \u001b[39m=\u001b[39m []\n\u001b[1;32m    113\u001b[0m \u001b[39mfor\u001b[39;00m activation, quantizer \u001b[39min\u001b[39;00m \\\n\u001b[1;32m    114\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquantize_config\u001b[39m.\u001b[39mget_activations_and_quantizers(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer):\n\u001b[0;32m--> 115\u001b[0m   quantize_activation \u001b[39m=\u001b[39m quantize_aware_activation\u001b[39m.\u001b[39;49mQuantizeAwareActivation(\n\u001b[1;32m    116\u001b[0m       activation, quantizer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer_step, \u001b[39mself\u001b[39;49m)\n\u001b[1;32m    118\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_quantize_activations\u001b[39m.\u001b[39mappend(quantize_activation)\n\u001b[1;32m    120\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_quantizers \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquantize_config\u001b[39m.\u001b[39mget_output_quantizers(\n\u001b[1;32m    121\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow_model_optimization/python/core/quantization/keras/quantize_aware_activation.py:109\u001b[0m, in \u001b[0;36mQuantizeAwareActivation.__init__\u001b[0;34m(self, activation, quantizer, step, quantize_wrapper)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquantize_wrapper \u001b[39m=\u001b[39m quantize_wrapper\n\u001b[1;32m    108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_supported_activation(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation):\n\u001b[0;32m--> 109\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_CUSTOM_ACTIVATION_ERR_MSG)\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_pre_quantize():\n\u001b[1;32m    112\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pre_activation_vars \u001b[39m=\u001b[39m quantizer\u001b[39m.\u001b[39mbuild(\u001b[39mNone\u001b[39;00m, \u001b[39m'\u001b[39m\u001b[39mpre_activation\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    113\u001b[0m                                               quantize_wrapper)\n",
      "\u001b[0;31mValueError\u001b[0m: Only some Keras activations under `tf.keras.activations` are supported. For other activations, use `Quantizer` directly, and update layer config using `QuantizeConfig`."
     ]
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "quantize_model = tfmot.quantization.keras.quantize_model\n",
    "\n",
    "# q_aware stands for for quantization aware.\n",
    "q_aware_model = quantize_model(dense_model)\n",
    "\n",
    "# `quantize_model` requires a recompile.\n",
    "q_aware_model.compile(optimizer='rmsprop',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "q_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.0420 - accuracy: 0.9900 - val_loss: 0.0654 - val_accuracy: 0.9853\n",
      "Epoch 2/10\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.0269 - accuracy: 0.9942 - val_loss: 0.0725 - val_accuracy: 0.9840\n",
      "Epoch 3/10\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.0261 - accuracy: 0.9952 - val_loss: 0.0681 - val_accuracy: 0.9880\n",
      "Epoch 4/10\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.0243 - accuracy: 0.9964 - val_loss: 0.0742 - val_accuracy: 0.9847\n",
      "Epoch 5/10\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.0231 - accuracy: 0.9962 - val_loss: 0.0782 - val_accuracy: 0.9860\n",
      "Epoch 6/10\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.0225 - accuracy: 0.9967 - val_loss: 0.0743 - val_accuracy: 0.9840\n",
      "Epoch 7/10\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.0219 - accuracy: 0.9964 - val_loss: 0.0718 - val_accuracy: 0.9873\n",
      "Epoch 8/10\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.0229 - accuracy: 0.9962 - val_loss: 0.0747 - val_accuracy: 0.9873\n",
      "Epoch 9/10\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.0212 - accuracy: 0.9968 - val_loss: 0.0844 - val_accuracy: 0.9853\n",
      "Epoch 10/10\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.0203 - accuracy: 0.9967 - val_loss: 0.0860 - val_accuracy: 0.9867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f89c4525430>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_aware_model.fit(x_train, y_train,\n",
    "                  batch_size=100, epochs=10, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline test accuracy: 0.9766666889190674\n",
      "Quant test accuracy: 0.9800000190734863\n"
     ]
    }
   ],
   "source": [
    "_, baseline_model_accuracy = dense_model.evaluate(\n",
    "    x_test, y_test, verbose=0)\n",
    "\n",
    "_, q_aware_model_accuracy = q_aware_model.evaluate(\n",
    "   x_test, y_test, verbose=0)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy)\n",
    "print('Quant test accuracy:', q_aware_model_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as flatten_3_layer_call_fn, flatten_3_layer_call_and_return_conditional_losses, dense_6_layer_call_fn, dense_6_layer_call_and_return_conditional_losses, dense_7_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpzrggk5qn/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpzrggk5qn/assets\n",
      "/home/veysiadn/anaconda3/lib/python3.9/site-packages/tensorflow/lite/python/convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
      "2022-10-12 16:51:41.801367: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2022-10-12 16:51:41.801385: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2022-10-12 16:51:41.801477: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpzrggk5qn\n",
      "2022-10-12 16:51:41.802486: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2022-10-12 16:51:41.802499: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/tmpzrggk5qn\n",
      "2022-10-12 16:51:41.806817: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2022-10-12 16:51:41.831836: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /tmp/tmpzrggk5qn\n",
      "2022-10-12 16:51:41.837958: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 36480 microseconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6984"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "quantized_tflite_model = converter.convert()\n",
    "tflite_model_file = pathlib.Path('dense_model_qat_training.tflite')\n",
    "tflite_model_file.write_bytes(quantized_tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate_model(interpreter):\n",
    "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "  # Run predictions on every image in the \"test\" dataset.\n",
    "  prediction_output = []\n",
    "  for i, test_x in enumerate(x_test):\n",
    "    test_x = np.expand_dims(test_x, axis=0).astype(np.float32)\n",
    "    interpreter.set_tensor(input_index, test_x)\n",
    "\n",
    "    # Run inference.\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Post-processing: remove batch dimension and find the digit with highest\n",
    "    # probability.\n",
    "    output = interpreter.tensor(output_index)\n",
    "    digit = np.argmax(output()[0])\n",
    "    prediction_output.append(digit)\n",
    "\n",
    "  print('\\n')\n",
    "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "  prediction_output = np.array(prediction_output)\n",
    "  output_predictions = np.expand_dims(prediction_output, axis=1)\n",
    "  accuracy = (output_predictions == y_test).mean()\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Quant TFLite test_accuracy: 0.98\n",
      "Quant TF test accuracy: 0.9800000190734863\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=quantized_tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "test_accuracy = evaluate_model(interpreter)\n",
    "\n",
    "print('Quant TFLite test_accuracy:', test_accuracy)\n",
    "print('Quant TF test accuracy:', q_aware_model_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save dense model as tflite - no optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpgad4k7bw/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-12 16:23:40.817925: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2022-10-12 16:23:40.817945: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2022-10-12 16:23:40.818301: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpgad4k7bw\n",
      "2022-10-12 16:23:40.818835: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2022-10-12 16:23:40.818847: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/tmpgad4k7bw\n",
      "2022-10-12 16:23:40.820638: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "2022-10-12 16:23:40.821070: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2022-10-12 16:23:40.840386: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /tmp/tmpgad4k7bw\n",
      "2022-10-12 16:23:40.844154: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 25855 microseconds.\n",
      "2022-10-12 16:23:40.851471: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18164"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_converter = tf.lite.TFLiteConverter.from_keras_model(dense_model)\n",
    "float_tflite_model = float_converter.convert()\n",
    "import pathlib\n",
    "tflite_model_file = pathlib.Path('dense_model.tflite')\n",
    "tflite_model_file.write_bytes(float_tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!xxd -i dense_model.tflite > dense_model.cc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantize dense model and save it as tflite.\n",
    "#### Full integer quantization of weights and activations :\n",
    "\n",
    "<a href='https://www.tensorflow.org/model_optimization/guide/quantization/post_training'> Check this Link</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpxbc41h1j/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpxbc41h1j/assets\n",
      "/home/veysiadn/anaconda3/lib/python3.9/site-packages/tensorflow/lite/python/convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
      "2022-10-12 16:26:58.561527: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2022-10-12 16:26:58.561541: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2022-10-12 16:26:58.561620: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpxbc41h1j\n",
      "2022-10-12 16:26:58.562096: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2022-10-12 16:26:58.562106: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/tmpxbc41h1j\n",
      "2022-10-12 16:26:58.563925: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2022-10-12 16:26:58.580900: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /tmp/tmpxbc41h1j\n",
      "2022-10-12 16:26:58.584490: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 22870 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7048"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def representative_data_gen():\n",
    "  for input_value in tf.data.Dataset.from_tensor_slices(x_val).batch(1).take(100):\n",
    "    # Model has only one input so each data point has one element.\n",
    "    yield [input_value]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(dense_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "tflite_model_quant = converter.convert()\n",
    "\n",
    "tflite_model_file = pathlib.Path('dense_model_optimized.tflite')\n",
    "tflite_model_file.write_bytes(tflite_model_quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!xxd -i dense_model_optimized.tflite > dense_model_optimized.cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp745zhh_m/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp745zhh_m/assets\n",
      "2022-10-12 16:30:58.904964: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2022-10-12 16:30:58.904998: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2022-10-12 16:30:58.905162: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmp745zhh_m\n",
      "2022-10-12 16:30:58.905935: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2022-10-12 16:30:58.905951: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/tmp745zhh_m\n",
      "2022-10-12 16:30:58.908250: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2022-10-12 16:30:58.928513: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /tmp/tmp745zhh_m\n",
      "2022-10-12 16:30:58.932586: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 27424 microseconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7048"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(dense_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.target_spec.supported_types = [tf.float32]\n",
    "converter.inference_input_type = tf.float32  # or tf.uint8\n",
    "converter.inference_output_type = tf.float32  # or tf.uint8\n",
    "tflite_quant_model = converter.convert()\n",
    "tflite_model_file = pathlib.Path('dense_model_full_integer_opt.tflite')\n",
    "tflite_model_file.write_bytes(tflite_model_quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "!xxd -i dense_model_full_integer_opt.tflite > dense_model_full_integer_opt.cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  <class 'numpy.float32'>\n",
      "output:  <class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path='dense_model.tflite')\n",
    "input_type = interpreter.get_input_details()[0]['dtype']\n",
    "print('input: ', input_type)\n",
    "output_type = interpreter.get_output_details()[0]['dtype']\n",
    "print('output: ', output_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to run inference on a TFLite model\n",
    "def run_tflite_model(tflite_file, indices):\n",
    "  # Initialize the interpreter\n",
    "  interpreter = tf.lite.Interpreter(model_path=str(tflite_file))\n",
    "  interpreter.allocate_tensors()\n",
    "\n",
    "  input_details = interpreter.get_input_details()[0]\n",
    "  output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "  predictions = 0\n",
    "  for i, index in enumerate(indices):\n",
    "    test_data = x_test[index]\n",
    "    test_data=np.expand_dims(test_data, axis=0)\n",
    "    test_labels = y_train[index]\n",
    "\n",
    "    interpreter.set_tensor(input_details[\"index\"], test_data)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "    print(output[0])\n",
    "    print(output[1])\n",
    "    print(output[2])\n",
    "    predictions = output.argmax()\n",
    "\n",
    "  return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.737704e-08\n",
      "0.00016263525\n",
      "0.99983716\n",
      " Model \n",
      " True:[2], Predicted:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "tflite_model_file = 'dense_model.tflite'\n",
    "# Change this to test a different image\n",
    "indice = 333\n",
    "\n",
    "## Helper function to test the models on one image\n",
    "def test_model(tflite_file, indice, model_type):\n",
    "\n",
    "  predictions = run_tflite_model(tflite_file, [indice])\n",
    "\n",
    "  print(f\" Model \\n True:{y_test[indice]}, Predicted:{predictions}\")\n",
    "  \n",
    "test_model(tflite_model_file, indice, model_type=\"Float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.lite.experimental.Analyzer.analyze(model_path='float_model.tflite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def run_tflite_model_for_eval(interpreter, indices):\n",
    "  input_details = interpreter.get_input_details()[0]\n",
    "  output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "  predictions = 0\n",
    "  for i, index in enumerate(indices):\n",
    "    test_data = x_test_to_use[index]\n",
    "    test_data=np.expand_dims(test_data, axis=0)\n",
    "\n",
    "    interpreter.set_tensor(input_details[\"index\"], test_data)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "    predictions = output.argmax()\n",
    "  return predictions\n",
    "  \n",
    "def evaluate_model(interpreter):\n",
    "  output_predictions =[]\n",
    "  for i in range(len(x_test_to_use)):\n",
    "    output_predictions.append(run_tflite_model_for_eval(interpreter,[i]))\n",
    "  output_predictions = np.array(output_predictions)\n",
    "  output_predictions = np.expand_dims(output_predictions, axis=1)\n",
    "  accuracy = (output_predictions == y_test_to_use).mean()\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized dense_model TFLite test_accuracy: 0.9826666666666667\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path='dense_model_optimized.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "test_accuracy = evaluate_model(interpreter)\n",
    "\n",
    "print('Optimized dense_model TFLite test_accuracy:', test_accuracy)\n",
    "# print('Quant TF test accuracy:', q_aware_model_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with annotated layers for quantization -- Trial can be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Quantization based training\n",
    "# import tensorflow_model_optimization as tfmot\n",
    "# quantize_annotate_layer = tfmot.quantization.keras.quantize_annotate_layer\n",
    "\n",
    "# model = tf.keras.Sequential([\n",
    "# #### tf.keras.layers :\n",
    "\n",
    "# quantize_annotate_layer((tf.keras.layers.Reshape((1000,1,1),name='Reshape', input_shape=(1000,)))),\n",
    "# quantize_annotate_layer(tf.keras.layers.Conv2D(16,7,padding='same')),\n",
    "# quantize_annotate_layer(tf.keras.layers.Activation('relu')),\n",
    "# quantize_annotate_layer(tf.keras.layers.MaxPooling2D(4,4,padding='same')),\n",
    "# quantize_annotate_layer(tf.keras.layers.Conv2D(8,5, padding='same')),\n",
    "# quantize_annotate_layer(tf.keras.layers.Activation('relu')),\n",
    "# quantize_annotate_layer(tf.keras.layers.MaxPooling2D(4,4,padding='same')),\n",
    "# quantize_annotate_layer(tf.keras.layers.MaxPooling2D(2,2,padding='same')),\n",
    "# quantize_annotate_layer(tf.keras.layers.Flatten()),\n",
    "# quantize_annotate_layer(tf.keras.layers.Dense(32)),\n",
    "# quantize_annotate_layer(tf.keras.layers.Dense(3, activation='softmax')),\n",
    "# ])\n",
    "# quantized_model = tfmot.quantization.keras.quantize_apply(model)\n",
    "# # quant_aware_model = tfmot.quantization.keras.quantize_model(base_model)\n",
    "# # quant_aware_model.summary()\n",
    "# print(model.summary())\n",
    "# quantized_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "#               loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# history = quantized_model.fit(x_train, y_train ,epochs=50, validation_data=(x_test, y_test), verbose=1)   \n",
    "\n",
    "\n",
    "# # Plotting accuracy and loss\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['accuracy'])\n",
    "# plt.plot(history.history['val_accuracy'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "\n",
    "\n",
    "# plt.title('Model loss & accuracy')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend(['tr_loss', 'tr_accuracy', 'val_acc', 'val_loss'], loc='lower right')\n",
    "# # accuracy!\n",
    "# print(\"Training results\")\n",
    "# print(f\"Accuracy: {history.history['accuracy'][-1]}\")\n",
    "# print(f\"Loss: {history.history['loss'][-1]}\")\n",
    "\n",
    "# # evaluating model\n",
    "# print(\"Evaluation results\")\n",
    "# print(f\"Accuracy: {history.history['val_accuracy'][-1]}\")\n",
    "# print(f\"Loss: {history.history['val_loss'][-1]}\")\n",
    "\n",
    "\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "41e74be16d15307d9f039f42bdba433d4433ec6233894682bf41214f89ea7b7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
